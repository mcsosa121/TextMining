Monday (8/24/17) notes
python -i topicmodel.py sagas_en.txt 20

when the probabilities are the same will give you the max value
entropy(np.array([1/3,1/3,1/3]))
1.5849625007211561
When probabilities are diffrent (as diffrent as possible) will give you the smallest value
entropy(np.array([99999999/100000000,0.000000001,0]))
3.1340048109443602e-08

entropy(np.array([1,0,0]))
0.0
 entropy(np.array([0,1,0]))
0.0
As long as completely deterministic distrubtion
No matter how you order the numbers (as long as they are the same) they will give you the same number
Uniform means every outcome is equally probable. 

Max entropy for distrubtion of 4 is 2. 
For 8 elements you get 3. 
Log base 2. I.e 2^x = (# elements) is the max value of entropy that you can get. 
If you double the number of elements the max value goes up by 1.

math.log2 
If all the same, returns the math log of the number of elements.
If only one non-zero, and all the rest are 0, then the (because of jensens inequality) will return the lowest possible
value which is zero. 


This prints out the document stuff
documents = documents[1160]
document
In the viking days 2 types of killing. If they insult you or sing a funny song about you then you go and kill them and 
tell their family, it was I who killed them. And then your family goes to theirs and decides how much 
you should pay them. The other type of killing is secret killing which is a no no.

document["topic_counts"]
array([ 1.,  1.,  1.,  0.,  0.,  1.,  2.,  0.,  2.,  0.,  1.,  0.,  1.,
        1.,  0.,  1.,  0.,  1.,  0.,  0.])

entropy(doc_topic_counts)
3.293
len(doc_topic_counts)
20
math.log2(20)
4.32192....

doc_topic_counts + doc_smoothing)

array([ 1.5,  1.5,  1.5,  0.5,  0.5,  1.5,  2.5,  0.5,  2.5,  0.5,  1.5,
        0.5,  1.5,  1.5,  0.5,  1.5,  0.5,  1.5,  0.5,  0.5])

Smothing. Moved the distrubtion away from something more chunky to something more smooth which is why 
we call it smoothing. 

entropy(doc_topic_counts + doc_smoothing)
4.0884909023973419

*Warning. The next thing will take a while. It is sampling*
sample(25)
[ 1.  1.  1.  0.  0.  1.  2.  0.  2.  0.  1.  0.  1.  1.  0.  1.  0.  1.
  0.  0.]
[ 0.  0.  0.  4.  0.  0.  2.  0.  1.  0.  2.  1.  3.  0.  0.  0.  0.  0.
  0.  0.]
[ 0.  0.  1.  6.  0.  0.  1.  0.  0.  2.  0.  0.  2.  0.  1.  0.  0.  0.
  0.  0.]
[ 0.  0.  3.  2.  0.  0.  0.  0.  0.  4.  0.  0.  1.  0.  0.  2.  0.  0.
  0.  1.]
[ 0.  0.  1.  0.  0.  0.  0.  0.  0.  2.  0.  0.  3.  0.  0.  4.  0.  0.
  2.  1.]
[ 0.  1.  0.  0.  0.  0.  0.  6.  0.  0.  0.  0.  4.  0.  0.  1.  0.  0.
  1.  0.]
[ 0.  1.  2.  0.  0.  0.  1.  2.  1.  0.  0.  0.  3.  0.  0.  1.  0.  0.
  2.  0.]
[ 1.  0.  0.  0.  0.  0.  1.  3.  0.  0.  0.  0.  2.  0.  0.  0.  0.  0.
  4.  2.]
[ 1.  0.  1.  0.  0.  0.  1.  1.  0.  0.  1.  0.  5.  0.  0.  0.  0.  0.
  1.  2.]
[ 4.  0.  0.  0.  0.  0.  0.  3.  1.  0.  0.  0.  2.  0.  0.  0.  1.  0.
  2.  0.]
[ 2.  1.  0.  0.  1.  0.  1.  2.  0.  0.  1.  0.  0.  1.  0.  0.  0.  0.
  4.  0.]
[ 3.  3.  0.  0.  1.  0.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.
  4.  0.]
[ 5.  0.  0.  1.  2.  0.  0.  0.  0.  0.  0.  0.  2.  0.  0.  0.  0.  0.
  3.  0.]
[ 5.  1.  0.  0.  1.  0.  1.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.
  4.  0.]
[ 2.  0.  2.  0.  1.  0.  0.  1.  0.  0.  0.  1.  1.  0.  0.  0.  0.  1.
  4.  0.]
[ 4.  0.  0.  1.  1.  0.  0.  1.  0.  0.  0.  0.  1.  0.  0.  0.  0.  1.
  4.  0.]
[ 5.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  1.  1.  0.  0.  1.  0.
  4.  0.]
[ 4.  0.  0.  0.  1.  0.  1.  0.  0.  1.  0.  0.  1.  1.  0.  0.  0.  0.
  3.  1.]
[ 4.  0.  0.  0.  1.  0.  1.  0.  0.  1.  0.  1.  1.  1.  0.  0.  0.  0.
  3.  0.]
[ 4.  0.  0.  0.  0.  0.  1.  0.  0.  1.  1.  0.  1.  1.  0.  0.  0.  0.
  3.  1.]
[ 3.  0.  0.  0.  2.  0.  0.  1.  0.  1.  0.  1.  2.  1.  0.  0.  0.  0.
  2.  0.]
[ 0.  0.  0.  0.  1.  0.  0.  2.  0.  1.  0.  3.  2.  0.  0.  0.  1.  1.
  2.  0.]
[ 2.  0.  0.  1.  1.  0.  0.  0.  1.  0.  0.  1.  2.  1.  0.  0.  0.  0.
  4.  0.]
[ 3.  0.  0.  0.  0.  0.  0.  0.  0.  2.  0.  0.  3.  1.  0.  0.  1.  0.
  3.  0.]
[ 4.  0.  0.  1.  1.  0.  1.  0.  0.  0.  0.  0.  2.  1.  0.  0.  0.  0.
  3.  0.]

entropy(doc_topic_counts + doc_smoothing)
3.6931962626138044
Recalculating you've gone down 
print_all_topics()
"You might be suprised but the viking liked sailing" LOL

1. How often does it occur in the document
2. How often does it occur in each topic.

Try to unpack what functions were from the entropy that you've gotten.
