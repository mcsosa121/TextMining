Monday (10/23/17) notes
python -i topicmodel.py sagas_en.txt 20

when the probabilities are the same will give you the max value
entropy(np.array([1/3,1/3,1/3]))
1.5849625007211561
When probabilities are diffrent (as diffrent as possible) will give you the smallest value
entropy(np.array([99999999/100000000,0.000000001,0]))
3.1340048109443602e-08

entropy(np.array([1,0,0]))
0.0
 entropy(np.array([0,1,0]))
0.0
As long as completely deterministic distrubtion
No matter how you order the numbers (as long as they are the same) they will give you the same number
Uniform means every outcome is equally probable. 

Max entropy for distrubtion of 4 is 2. 
For 8 elements you get 3. 
Log base 2. I.e 2^x = (# elements) is the max value of entropy that you can get. 
If you double the number of elements the max value goes up by 1.

math.log2 
If all the same, returns the math log of the number of elements.
If only one non-zero, and all the rest are 0, then the (because of jensens inequality) will return the lowest possible
value which is zero. 


This prints out the document stuff
documents = documents[1160]
document
In the viking days 2 types of killing. If they insult you or sing a funny song about you then you go and kill them and 
tell their family, it was I who killed them. And then your family goes to theirs and decides how much 
you should pay them. The other type of killing is secret killing which is a no no.

document["topic_counts"]
array([ 1.,  1.,  1.,  0.,  0.,  1.,  2.,  0.,  2.,  0.,  1.,  0.,  1.,
        1.,  0.,  1.,  0.,  1.,  0.,  0.])

entropy(doc_topic_counts)
3.293
len(doc_topic_counts)
20
math.log2(20)
4.32192....

doc_topic_counts + doc_smoothing)

array([ 1.5,  1.5,  1.5,  0.5,  0.5,  1.5,  2.5,  0.5,  2.5,  0.5,  1.5,
        0.5,  1.5,  1.5,  0.5,  1.5,  0.5,  1.5,  0.5,  0.5])

Smothing. Moved the distrubtion away from something more chunky to something more smooth which is why 
we call it smoothing. 

entropy(doc_topic_counts + doc_smoothing)
4.0884909023973419

*Warning. The next thing will take a while. It is sampling*
sample(25)
[ 1.  1.  1.  0.  0.  1.  2.  0.  2.  0.  1.  0.  1.  1.  0.  1.  0.  1.
  0.  0.]
[ 0.  0.  0.  4.  0.  0.  2.  0.  1.  0.  2.  1.  3.  0.  0.  0.  0.  0.
  0.  0.]
[ 0.  0.  1.  6.  0.  0.  1.  0.  0.  2.  0.  0.  2.  0.  1.  0.  0.  0.
  0.  0.]
[ 0.  0.  3.  2.  0.  0.  0.  0.  0.  4.  0.  0.  1.  0.  0.  2.  0.  0.
  0.  1.]
[ 0.  0.  1.  0.  0.  0.  0.  0.  0.  2.  0.  0.  3.  0.  0.  4.  0.  0.
  2.  1.]
[ 0.  1.  0.  0.  0.  0.  0.  6.  0.  0.  0.  0.  4.  0.  0.  1.  0.  0.
  1.  0.]
[ 0.  1.  2.  0.  0.  0.  1.  2.  1.  0.  0.  0.  3.  0.  0.  1.  0.  0.
  2.  0.]
[ 1.  0.  0.  0.  0.  0.  1.  3.  0.  0.  0.  0.  2.  0.  0.  0.  0.  0.
  4.  2.]
[ 1.  0.  1.  0.  0.  0.  1.  1.  0.  0.  1.  0.  5.  0.  0.  0.  0.  0.
  1.  2.]
[ 4.  0.  0.  0.  0.  0.  0.  3.  1.  0.  0.  0.  2.  0.  0.  0.  1.  0.
  2.  0.]
[ 2.  1.  0.  0.  1.  0.  1.  2.  0.  0.  1.  0.  0.  1.  0.  0.  0.  0.
  4.  0.]
[ 3.  3.  0.  0.  1.  0.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.
  4.  0.]
[ 5.  0.  0.  1.  2.  0.  0.  0.  0.  0.  0.  0.  2.  0.  0.  0.  0.  0.
  3.  0.]
[ 5.  1.  0.  0.  1.  0.  1.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.
  4.  0.]
[ 2.  0.  2.  0.  1.  0.  0.  1.  0.  0.  0.  1.  1.  0.  0.  0.  0.  1.
  4.  0.]
[ 4.  0.  0.  1.  1.  0.  0.  1.  0.  0.  0.  0.  1.  0.  0.  0.  0.  1.
  4.  0.]
[ 5.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  1.  1.  0.  0.  1.  0.
  4.  0.]
[ 4.  0.  0.  0.  1.  0.  1.  0.  0.  1.  0.  0.  1.  1.  0.  0.  0.  0.
  3.  1.]
[ 4.  0.  0.  0.  1.  0.  1.  0.  0.  1.  0.  1.  1.  1.  0.  0.  0.  0.
  3.  0.]
[ 4.  0.  0.  0.  0.  0.  1.  0.  0.  1.  1.  0.  1.  1.  0.  0.  0.  0.
  3.  1.]
[ 3.  0.  0.  0.  2.  0.  0.  1.  0.  1.  0.  1.  2.  1.  0.  0.  0.  0.
  2.  0.]
[ 0.  0.  0.  0.  1.  0.  0.  2.  0.  1.  0.  3.  2.  0.  0.  0.  1.  1.
  2.  0.]
[ 2.  0.  0.  1.  1.  0.  0.  0.  1.  0.  0.  1.  2.  1.  0.  0.  0.  0.
  4.  0.]
[ 3.  0.  0.  0.  0.  0.  0.  0.  0.  2.  0.  0.  3.  1.  0.  0.  1.  0.
  3.  0.]
[ 4.  0.  0.  1.  1.  0.  1.  0.  0.  0.  0.  0.  2.  1.  0.  0.  0.  0.
  3.  0.]

entropy(doc_topic_counts + doc_smoothing)
3.6931962626138044
Recalculating you've gone down 
print_all_topics()
"You might be suprised but the viking liked sailing" LOL

1. How often does it occur in the document
2. How often does it occur in each topic.

Try to unpack what functions were from the entropy that you've gotten.

10/25/17 notes
What does roman science look like? (What do you think)
I think it is very based on the gods and around psuedoscience. However, I do believe there
will be a good amount of actual science infused in there. For example pythagoras was 
also old as fuck but he knew how to draw triangles and approximate stuff. 

Probably mathematics, zooology, civic engineering, other things

Our topic was coherrence
  This metric measures whether the words in a topic tend to co-occur together. We add up a score for each distinct pair of top ranked words.
  The score is the log of the probability that a document containing at least one instance of the higher-ranked word also contains at least one
  instance of the lower-ranked word.
    ∑i∑j<ilogD(wj,wi)+βD(wi)
    ∑i∑j<ilog⁡D(wj,wi)+βD(wi)
  To avoid log zero errors we add the "beta" topic-word smoothing parameter specified when you calculate diagnostics. Since these scores are log
  probabilities they are negative. Large negative values indicate words that don't co-occur often; values closer to zero indicate that words tend
  to co-occur more often. The least coherent topic in the sample file is 
    "polish poland danish denmark sweden swedish na norway norwegian sk red iceland bj baltic copenhagen cave greenland krak gda faroese". 
  This topic seems to be about Northern and Eastern European countries, but the short abbreviations "na" and "sk" and the words "red" and "cave"
  don't really fit.

Model 1
The below topic is quite low on a Coherence x Token plot 
14. art invented plants according magic himself medicine discovered arts use son practice mankind man medical properties hellebore treatment physicians discovery
24. wheat seed grain plants sown barley bread food millet bean garden stalk cabbage sowing rape grains corn spelt garlic meal


The token below was pretty high
30. nature much thus still itself great though every without what already certain now must find make never name various can



If red has low probability of occuring with one of the more highly ranked words. (I.e the word you see and go whatttt)
ie. in Model 5
105. cabbage tiber latium cato circeii exist result umbri both praises surrentum line sprouts cymæ fidenates arretini tusci sabines sung level
"tiber", "latium",... etc. are red. 

Model 5
90. wine boiled water decoction bowels employed drink doses stomach affections juice vinegar pains leaves effect root plant cure seed acts

Generally similar across models with some sub genre variation

So its the words that occur together. If more topics, higher coherence within each topic because highly specialized

Rank 1 document means when topics occur they occur a lot.
Low entropy means only in a few of the focuments while high entropy means in a lot of the documents.
