Python commands to run
"python -i lsa.py"
file_word_counts.shape
weights.shape
word_vectors.shape
weights
>>> array([ 0.77857381,  0.1442594 ,  0.09684055,  0.07997597,  0.06565675,
        0.05952633,  0.05503083,  0.04978523,  0.04889757,  0.04367355,
        0.0423832 ,  0.03753432,  0.03412163,  0.03345046,  0.0312974 ,
        0.02892366,  0.02883309,  0.02752158,  0.02564249,  0.02515039,
        0.02487118,  0.02406178,  0.02296391,  0.02283825,  0.02201972,
        0.02166528,  0.02089765,  0.02053535,  0.01994015,  0.0193352 ,
        0.01872818,  0.0183326 ,  0.01815518,  0.01776288,  0.01749864,
        0.01709079,  0.01659644,  0.01616356,  0.01594229,  0.01529915,
        0.01512998,  0.0148049 ,  0.0144487 ,  0.01433389,  0.01349184,
        0.01342357,  0.01325588,  0.01314029,  0.01309045,  0.01231284,
        0.01207496,  0.01159215,  0.01153071,  0.01143738,  0.01078014,
        0.01064879,  0.01017267,  0.01007114,  0.00989446,  0.00969715,
        0.00954573,  0.00948953,  0.00923773,  0.00878618,  0.00870806,
        0.00803201,  0.00759423,  0.00727878,  0.00659901,  0.00534127])
First vector whose weight is 0.7785 .... has more value over other vectors with weights.
Want to find vector for "the"
can look at vocabulary[:10] or vocabulary.index("the"). The index is 0.
word_vectors[0,:]
zip(vocabulary, word_vectors[:,0])
list(zip(vocabulary,word_vectors[:,0]))[:10]

Friday:
sort_vector(word_vectors[:,0], vocabulary)
(-0.58784424104047894, 'the')
(-0.36017932948639075, 'and')
(-0.30653754561790902, 'of')
(-0.29116758103800722, 'to')
(-0.23100808469243636, 'a')
(-0.19142062600828455, 'I')
(-0.16835086229106269, 'in')
(-0.13434607749615149, 'that')
(-0.13041940269807975, 'was')
(-0.12187193816277338, 'he')
(-1.629524590526345e-05, 'Plumet')
(-1.6016220295213072e-05, 'Champs-Élysées')
(-1.5969340987158217e-05, 'Théodule')
(-1.5643436069052909e-05, 'Petit-Picpus')
(-1.5105357932930828e-05, 'grape-shot')
(-1.499162623284239e-05, 'Toussaint')
(-1.4665721314737079e-05, 'Babet')
(-1.4665721314737079e-05, 'Chanvrerie')
(-1.4339816396631826e-05, 'Boulatruelle')
(-1.4339816396631826e-05, 'Champmathieu')
If you want to get the single best approximaxtion of english novels then sort them in order or frequency

sort_vector(word_vectors[:,1], vocabulary)
(-0.59970178597414758, 'I')
(-0.28672641258842629, 'you')
(-0.22443118335314224, 'my')
(-0.18765644265378914, 'me')
(-0.13773180521669229, 'to')
(-0.11715089382546755, 'her')
(-0.10833477520372613, 'not')
(-0.10567202630267278, 'it')
(-0.096850913731791069, 'is')
(-0.096719249968536161, 'your')
(0.022698421157209873, 'were')
(0.023281839756680949, 'which')
(0.023919984791398074, 'they')
(0.027597097641101184, 'their')
(0.037932929673543649, 'fisherman')
(0.05479105451994868, 'he')
(0.055543405182502323, 'The')
(0.072509659207097843, 'his')
(0.14435099547035818, 'of')
(0.47758603142203532, 'the')

Can do the same thing but with titles
sort_vector(word_vectors[:,1], titles)
(-0.59970178597414758, "Alice's Adventures in Wonderland")
(-0.28672641258842629, 'Ethan Frome')
(-0.22443118335314224, 'On the Duty of Civil Disobediance')
(-0.18765644265378914, 'Othello')
(-0.13773180521669229, 'Adventures of Huckleberry Finn')
(-0.11715089382546755, 'Heart of Darkness')
(-0.10833477520372613, 'Little Women')
(-0.10567202630267278, 'Dracula')
(-0.096850913731791069, 'Les Misérables')
(-0.096719249968536161, 'The Taming of the Shrew')
(0.01666142153587211, 'Around the World in Eighty Days')
(0.022698421157209873, 'The Adventures of Sherlock Holmes')
(0.023281839756680949, 'Sense and Sensibility')
(0.023919984791398074, "Tess of the d'Urbervilles: A Pure Woman")
(0.027597097641101184, 'The Picture of Dorian Gray')
(0.05479105451994868, 'Candide')
(0.055543405182502323, 'Pride and Predjudice ')
(0.072509659207097843, 'Don Quixote')
(0.14435099547035818, 'A Tale of Two Cities')
(0.47758603142203532, 'A Christmas Carol')